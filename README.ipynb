{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic introduction to Anyscale and Airflow\n",
    "\n",
    "This introduction will cover the following topics:\n",
    "1. Setting up a local Airflow environment with the Anyscale provider installed.\n",
    "2. Running an example DAG to submit an Anyscale job.\n",
    "3. Running an example DAG to deploy an Anyscale service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Getting airflow up and running\n",
    "\n",
    "Let's start by setting up a local Airflow environment with the Anyscale provider installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 1: Fetch the default airflow `docker-compose.yaml` file\n",
    "\n",
    "Run the following command to fetch the default `docker-compose.yaml` file from the Apache Airflow repository:\n",
    "\n",
    "```bash\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.1.2/docker-compose.yaml'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 2: Creating a custom Airflow dockerfile\n",
    "\n",
    "Create a `Dockerfile` in the root directory of this repository with the following contents:\n",
    "\n",
    "```Dockerfile\n",
    "FROM apache/airflow:2.1.2\n",
    "\n",
    "RUN pip install astro-provider-anyscale==1.0.0\n",
    "```\n",
    "What this does is it creates a custom image based on the official Apache Airflow image and installs the `astro-provider-anyscale` package. \n",
    "\n",
    "This ensures that the Anyscale provider is available when Airflow executes the DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 3: Modify the `docker-compose.yaml` file to build the custom image\n",
    "\n",
    "Modify the `docker-compose.yaml` file to build the custom image.\n",
    "\n",
    "See this section of the `docker-compose.yaml` file:\n",
    "\n",
    "```yaml\n",
    "x-airflow-common:\n",
    "  &airflow-common\n",
    "  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n",
    "  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n",
    "  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n",
    "  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.2}\n",
    "  build: .\n",
    "```\n",
    "\n",
    "We comment out the `image` line and uncomment the `build` line. This tells docker-compose to build the image using the `Dockerfile` in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 4: Modify the `docker-compose.yaml` file to avoid loading example DAGs\n",
    "    \n",
    "To avoid loading the example DAGs, set the `AIRFLOW__CORE__LOAD_EXAMPLES` environment variable to `false`:\n",
    "\n",
    "```\n",
    "x-airflow-common:\n",
    "  ...\n",
    "  environment:\n",
    "    ...\n",
    "    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 5: Run `docker-compose up`\n",
    "Run `docker-compose up` in the root directory of this repository. This will start the airflow webserver and scheduler.\n",
    "\n",
    "You should see output that looks like this:\n",
    "```\n",
    "❯ docker-compose up\n",
    "[+] Running 7/0\n",
    " ✔ Container basic-demo-postgres-1           Running                                                                                                           0.0s \n",
    " ✔ Container basic-demo-redis-1              Running                                                                                                           0.0s \n",
    " ✔ Container basic-demo-airflow-init-1       Created                                                                                                           0.0s \n",
    " ✔ Container basic-demo-airflow-worker-1     Running                                                                                                           0.0s \n",
    " ✔ Container basic-demo-airflow-scheduler-1  Running                                                                                                           0.0s \n",
    " ✔ Container basic-demo-airflow-webserver-1  Running                                                                                                           0.0s \n",
    " ✔ Container basic-demo-airflow-triggerer-1  Running                                                                                                           0.0s \n",
    "Attaching to airflow-init-1, airflow-scheduler-1, airflow-triggerer-1, airflow-webserver-1, airflow-worker-1, postgres-1, redis-1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 6: Access the webserver\n",
    "You can access the webserver at `localhost:8080`.\n",
    "\n",
    "You should land on the login page if this is your first time accessing the webserver.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_sign_in.png\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 7: Log in\n",
    "The default username and password are `airflow` and `airflow` respectively as defined in the `docker-compose.yaml` file.\n",
    "\n",
    "```\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      POSTGRES_USER: airflow\n",
    "      POSTGRES_PASSWORD: airflow\n",
    "      POSTGRES_DB: airflow\n",
    "```\n",
    "\n",
    "So you can log in with the username `airflow` and password `airflow`.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_sign_in_full.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 8: View the available active DAGs\n",
    "\n",
    "You should see two DAGs available when you log in. These are the `sample_anyscale_job_workflow` and `sample_anyscale_service_workflow` DAGs.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_home.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running an example DAG to submit an Anyscale job.\n",
    "\n",
    "Now that we have the Airflow environment set up, let's run the `sample_anyscale_job_workflow` DAG.\n",
    "\n",
    "### Step 0: Going over the syntax of the DAG\n",
    "\n",
    "The DAG is defined in the `sample_anyscale_job_workflow.py` file in the `dags` directory.\n",
    "\n",
    "The DAG is defined as follows:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.anyscale.operators.anyscale import AnyscaleSubmitJobOperator\n",
    "\n",
    "...\n",
    "\n",
    "# consult the SDK documentation\n",
    "# https://docs.anyscale.com/reference/job-api#job-models\n",
    "anyscale_job_config = dict(\n",
    "    working_dir=str(FOLDER_PATH),\n",
    "    entrypoint=\"python ray_job.py\",\n",
    "    max_retries=1,\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "submit_anyscale_job = SubmitAnyscaleJob(\n",
    "    # base airflow operator parameters\n",
    "    task_id=\"submit_anyscale_job\",\n",
    "    dag=dag,\n",
    "    conn_id=ANYSCALE_CONN_ID,\n",
    "    name=\"Simple Anyscale Job\",\n",
    "    # custom operator parameters\n",
    "    wait_for_completion=True,\n",
    "    job_timeout_seconds=3000,\n",
    "    poll_interval=10,\n",
    "    # Anyscale Job Config\n",
    "    **anyscale_job_config,\n",
    ")\n",
    "```\n",
    "\n",
    "### Step 1: Trigger the DAG\n",
    "\n",
    "Click on the trigger DAG button on the `sample_anyscale_job_workflow` DAG.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_trigger_dag.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 2: Click on the DAG run \n",
    "\n",
    "Click on the DAG run to view a list of DAG runs.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_home_run.png\" width=\"700px\">\n",
    "\n",
    "### Step 3: View the DAG runs\n",
    "\n",
    "Click on the specific DAG run in the list.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_dag_runs.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 4: View the DAG run graph\n",
    "\n",
    "You should see a DAG run graph that looks like this:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_run_graph.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 5: View the task logs\n",
    "\n",
    "You can view the task logs by clicking on the task and then selecting the logs tab.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_dag_logs.png\" width=\"700px\">\n",
    "\n",
    "### Step 6: View the Anyscale job\n",
    "You can view the Anyscale job by clicking on the `Anyscale` link in the logs.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_jobs.png\" width=\"700px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running an example DAG to deploy an Anyscale service.\n",
    "\n",
    "Now that we have the Airflow environment set up, let's run the `sample_anyscale_service_workflow` DAG.\n",
    "\n",
    "### Step 0: Going over the syntax of the DAG\n",
    "\n",
    "The DAG is defined in the `sample_anyscale_service_workflow.py` file in the `dags` directory.\n",
    "\n",
    "The DAG is defined as follows:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.anyscale.operators.anyscale import AnyscaleSubmitJobOperator\n",
    "\n",
    "...\n",
    "\n",
    "# consult the SDK documentation\n",
    "# https://docs.anyscale.com/reference/service-api#service-models\n",
    "anyscale_service_config = dict(\n",
    "    working_dir=\"https://github.com/anyscale/docs_examples/archive/refs/heads/main.zip\",\n",
    "    applications=[{\"import_path\": \"sentiment_analysis.app:model\"}],\n",
    "    requirements=[\"transformers\", \"requests\", \"pandas\", \"numpy\", \"torch\"],\n",
    "    in_place=False,\n",
    "    canary_percent=None,\n",
    ")\n",
    "\n",
    "...\n",
    "\n",
    "deploy_anyscale_service = RolloutAnyscaleService(\n",
    "    # base airflow operator parameters\n",
    "    task_id=\"rollout_anyscale_service\",\n",
    "    conn_id=ANYSCALE_CONN_ID,\n",
    "    name=SERVICE_NAME,\n",
    "    dag=dag,\n",
    "    # custom operator parameters\n",
    "    service_rollout_timeout_seconds=600,\n",
    "    poll_interval=30,\n",
    "    # Anyscale Service Config\n",
    "    **anyscale_service_config,\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Step 1: Trigger the DAG\n",
    "\n",
    "Click on the trigger DAG button on the `sample_anyscale_service_workflow` DAG.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_service_trigger_dag.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 2: View the DAG run graph\n",
    "\n",
    "Similar to the previous section, you can follow the above steps until you see a DAG run graph that looks like this:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_service_run_graph.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 3: View the DAG runs\n",
    "\n",
    "Click on the specific DAG run in the list.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_service_run_list.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 4: View the DAG run graph\n",
    "\n",
    "You should see a DAG run graph that looks like this:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_service_run_graph.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 5: View the task logs\n",
    "\n",
    "You can view the task logs by clicking on the task and then selecting the logs tab.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/airflow_service_logs.png\" width=\"700px\">\n",
    "\n",
    "\n",
    "### Step 6: View the Anyscale service\n",
    "You can view the Anyscale service by clicking on the `Anyscale` link in the logs.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/airflow-demo/anyscale_service.png\" width=\"700px\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
